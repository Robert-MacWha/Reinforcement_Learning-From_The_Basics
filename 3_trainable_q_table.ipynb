{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd086c080258fc1895424e842bd7334418d491c3ba45b63b72b9209632f640ad29e",
   "display_name": "Python 3.9.5 64-bit ('rl': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "cadee6d9ca5a51ce1691b807cfbce535d2a09246bd6445805893caa7c00cfe39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? initialize the environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# get the action and observation space (used when constructing the q table)\n",
    "ACTION_SPACE      = env.action_space.n\n",
    "OBSERVATION_SPACE = len(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? build the q table\n",
    "Q_INCREMENTS = 20 # how detailed the q table is\n",
    "DISCRETE_OS_SIZE = [Q_INCREMENTS] * OBSERVATION_SPACE\n",
    "\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [ACTION_SPACE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? build a function that takes an observation and return the action given by the q table\n",
    "def obs_To_Index(env, obs, increments):\n",
    "\n",
    "    # get the bounds of the observation_space\n",
    "    obs_min = env.observation_space.low\n",
    "    obs_max = env.observation_space.high\n",
    "\n",
    "    # normalize the observation\n",
    "    obs = (obs - obs_min) / (obs_max - obs_min)\n",
    "\n",
    "    # convert the normalized array to an integer indice\n",
    "    indice = tuple(np.round(obs * increments).astype(int))\n",
    "\n",
    "    return indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? initialize parameters related to training\n",
    "EPOCHS        = 5000  # number of environments to simulate\n",
    "DISCOUNT      = 0.95  # how much the agent cares about future rewards\n",
    "LEARNING_RATE = 0.1   # how quickly values in the q table change\n",
    "\n",
    "RENDER_EVERY  = 500  # how often to render a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Won at epoch 579\n",
      "Won at epoch 584\n",
      "Won at epoch 591\n",
      "Won at epoch 593\n",
      "Won at epoch 594\n",
      "Won at epoch 598\n",
      "Won at epoch 600\n",
      "Won at epoch 601\n",
      "Won at epoch 602\n",
      "Won at epoch 604\n",
      "Won at epoch 610\n",
      "Won at epoch 611\n",
      "Won at epoch 613\n",
      "Won at epoch 618\n",
      "Won at epoch 619\n",
      "Won at epoch 672\n",
      "Won at epoch 689\n",
      "Won at epoch 691\n",
      "Won at epoch 714\n",
      "Won at epoch 720\n",
      "Won at epoch 724\n",
      "Won at epoch 725\n",
      "Won at epoch 730\n",
      "Won at epoch 731\n",
      "Won at epoch 733\n",
      "Won at epoch 734\n",
      "Won at epoch 783\n",
      "Won at epoch 784\n",
      "Won at epoch 799\n",
      "Won at epoch 815\n",
      "Won at epoch 832\n",
      "Won at epoch 842\n",
      "Won at epoch 848\n",
      "Won at epoch 852\n",
      "Won at epoch 853\n",
      "Won at epoch 861\n",
      "Won at epoch 878\n",
      "Won at epoch 880\n",
      "Won at epoch 882\n",
      "Won at epoch 885\n",
      "Won at epoch 895\n",
      "Won at epoch 909\n",
      "Won at epoch 913\n",
      "Won at epoch 915\n",
      "Won at epoch 944\n",
      "Won at epoch 954\n",
      "Won at epoch 960\n",
      "Won at epoch 961\n",
      "Won at epoch 988\n",
      "Won at epoch 998\n",
      "Won at epoch 1015\n",
      "Won at epoch 1016\n",
      "Won at epoch 1018\n",
      "Won at epoch 1087\n",
      "Won at epoch 1098\n",
      "Won at epoch 1099\n",
      "Won at epoch 1105\n",
      "Won at epoch 1107\n",
      "Won at epoch 1110\n",
      "Won at epoch 1112\n",
      "Won at epoch 1122\n",
      "Won at epoch 1124\n",
      "Won at epoch 1125\n",
      "Won at epoch 1128\n",
      "Won at epoch 1131\n",
      "Won at epoch 1133\n",
      "Won at epoch 1135\n",
      "Won at epoch 1141\n",
      "Won at epoch 1159\n",
      "Won at epoch 1160\n",
      "Won at epoch 1171\n",
      "Won at epoch 1172\n",
      "Won at epoch 1179\n",
      "Won at epoch 1180\n",
      "Won at epoch 1182\n",
      "Won at epoch 1183\n",
      "Won at epoch 1184\n",
      "Won at epoch 1186\n",
      "Won at epoch 1187\n",
      "Won at epoch 1190\n",
      "Won at epoch 1192\n",
      "Won at epoch 1202\n",
      "Won at epoch 1207\n",
      "Won at epoch 1208\n",
      "Won at epoch 1211\n",
      "Won at epoch 1215\n",
      "Won at epoch 1220\n",
      "Won at epoch 1224\n",
      "Won at epoch 1225\n",
      "Won at epoch 1226\n",
      "Won at epoch 1228\n",
      "Won at epoch 1231\n",
      "Won at epoch 1236\n",
      "Won at epoch 1239\n",
      "Won at epoch 1244\n",
      "Won at epoch 1247\n",
      "Won at epoch 1248\n",
      "Won at epoch 1249\n",
      "Won at epoch 1251\n",
      "Won at epoch 1252\n",
      "Won at epoch 1254\n",
      "Won at epoch 1263\n",
      "Won at epoch 1264\n",
      "Won at epoch 1265\n",
      "Won at epoch 1270\n",
      "Won at epoch 1273\n",
      "Won at epoch 1275\n",
      "Won at epoch 1278\n",
      "Won at epoch 1279\n",
      "Won at epoch 1287\n",
      "Won at epoch 1289\n",
      "Won at epoch 1291\n",
      "Won at epoch 1293\n",
      "Won at epoch 1298\n",
      "Won at epoch 1299\n",
      "Won at epoch 1310\n",
      "Won at epoch 1316\n",
      "Won at epoch 1317\n",
      "Won at epoch 1318\n"
     ]
    }
   ],
   "source": [
    "#? train the agent by updating the q table\n",
    "for e in range(1, EPOCHS+1):\n",
    "\n",
    "    # store the initial state of the environment\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "        \n",
    "    while not done:\n",
    "\n",
    "        # render every [RENDER_EVERY] epochs\n",
    "        if e % RENDER_EVERY == 0:\n",
    "            env.render()\n",
    "\n",
    "        # find the discrete cell coresponding to the current observation\n",
    "        indice = obs_To_Index(env, observation, Q_INCREMENTS)\n",
    "\n",
    "        # select the action to take\n",
    "        action = q_table[indice].argmax()\n",
    "\n",
    "        # take the action\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # calculate the predicted future reward\n",
    "        new_indice = obs_To_Index(env, new_observation, Q_INCREMENTS)\n",
    "        future_reward = reward + DISCOUNT * q_table[new_indice].max()\n",
    "\n",
    "        # update the value in the q table\n",
    "        current_q = q_table[indice + (action,)]\n",
    "        new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * future_reward\n",
    "\n",
    "        q_table[indice + (action,)] = new_q\n",
    "\n",
    "        # update the current observation\n",
    "        observation = new_observation\n",
    "\n",
    "        if (observation[0] >= env.goal_position):\n",
    "            print(f'Won at epoch {e}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}