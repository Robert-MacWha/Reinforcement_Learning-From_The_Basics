{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python395jvsc74a57bd086c080258fc1895424e842bd7334418d491c3ba45b63b72b9209632f640ad29e",
   "display_name": "Python 3.9.5 64-bit ('rl': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "cadee6d9ca5a51ce1691b807cfbce535d2a09246bd6445805893caa7c00cfe39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? initialize the environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# get the action and observation space (used when constructing the q table)\n",
    "ACTION_SPACE      = env.action_space.n\n",
    "OBSERVATION_SPACE = len(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? build the q table\n",
    "Q_INCREMENTS = 20 # how detailed the q table is\n",
    "DISCRETE_OS_SIZE = [Q_INCREMENTS] * OBSERVATION_SPACE\n",
    "\n",
    "q_table = np.random.uniform(low=-2, high=0, size=(DISCRETE_OS_SIZE + [ACTION_SPACE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? build a function that takes an observation and return the action given by the q table\n",
    "def obs_To_Index(env, obs, increments):\n",
    "\n",
    "    # get the bounds of the observation_space\n",
    "    obs_min = env.observation_space.low\n",
    "    obs_max = env.observation_space.high\n",
    "\n",
    "    # normalize the observation\n",
    "    obs = (obs - obs_min) / (obs_max - obs_min)\n",
    "\n",
    "    # convert the normalized array to an integer indice\n",
    "    indice = tuple(np.floor(obs * increments).astype(int))\n",
    "\n",
    "    return indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? initialize parameters related to training\n",
    "EPOCHS        = 5000  # number of environments to simulate\n",
    "DISCOUNT      = 0.95  # how much the agent cares about future rewards\n",
    "LEARNING_RATE = 0.1   # how quickly values in the q table change\n",
    "EPSILON       = 0.5   # chance of the agent taking a random action\n",
    "EPSILON_DECAY = 0.9998\n",
    "\n",
    "RENDER_EVERY  = 500  # how often to render a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? train the agent by updating the q table\n",
    "for e in range(1, EPOCHS+1):\n",
    "\n",
    "    # store the initial state of the environment\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "        \n",
    "    while not done:\n",
    "\n",
    "        # render every [RENDER_EVERY] epochs\n",
    "        if e % RENDER_EVERY == 0:\n",
    "            env.render()\n",
    "\n",
    "        # find the discrete cell coresponding to the current observation\n",
    "        indice = obs_To_Index(env, observation, Q_INCREMENTS)\n",
    "\n",
    "        # select the action to take\n",
    "        if random.uniform(0, 1) < EPSILON:\n",
    "            action = env.action_space.sample() # random action (exploration)\n",
    "        else:\n",
    "            action = q_table[indice].argmax()  # action from the q table\n",
    "\n",
    "        # take the action\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # calculate the predicted future reward\n",
    "        new_indice = obs_To_Index(env, new_observation, Q_INCREMENTS)\n",
    "        future_reward = reward + DISCOUNT * q_table[new_indice].max()\n",
    "\n",
    "        # update the value in the q table\n",
    "        current_q = q_table[indice + (action,)]\n",
    "        new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * future_reward\n",
    "\n",
    "        q_table[indice + (action,)] = new_q\n",
    "\n",
    "        # update the current observation\n",
    "        observation = new_observation\n",
    "\n",
    "    # reduce epsilon\n",
    "    EPSILON = EPSILON * EPSILON_DECAY\n",
    "\n",
    "env.close()\n",
    "\n",
    "# save the q table\n",
    "np.save('(3) trained_t_table.npy', q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}